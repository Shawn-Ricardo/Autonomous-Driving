{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n",
    "\n",
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the path to training, testing, and validation data\n",
    "\n",
    "training_file = \"traffic_data/train.p\"\n",
    "testing_file=\"traffic_data/test.p\"\n",
    "\n",
    "# read data into variables\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "# an input image will be of shape [32, 32, 3]. Width = 32, Height = 32, and Channels = 3\n",
    "x_train, y_train = np.array(train['features']), np.array(train['labels']) \n",
    "x_test, y_test = np.array(test['features']), np.array(test['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Obtain validation data set from training data set.Further preprocessing on images can be done here, as well. Including image normalization, color channel changes (to YUV, HSV, or Gray), and inverting input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make 20% of the training dataset part of the validation data set.\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, \n",
    "                                                      test_size = 0.2, \n",
    "                                                      random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Variables & Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define how many times the model will train on the training set.\n",
    "EPOCHS = 100\n",
    "\n",
    "# define how many images to run through the model at once.\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# total number of images in training set\n",
    "NUM_EXAMPLES = len(x_train)\n",
    "\n",
    "# TensorFlow variables.\n",
    "\n",
    "# Theses variables will be updated at run time by populating feed_dict\n",
    "\n",
    "# x_image will hold the traffic sign images to train on. \"None\" tells \n",
    "# tensorflow to example any size for this dimension. In this case,\n",
    "# None corresponds to the BATCH_SIZE\n",
    "x_image = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "\n",
    "# y_ will hold the labels for the corresponding images in x_image.\n",
    "y_ = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# y_ is an array of integers from 0 to 42. In order for the network to optimize itself\n",
    "# (discussed below), the labels array must be one-hot encoded. Meaning each entry in\n",
    "# y_ will be replaced with a vector that contains all zeros except at the index that \n",
    "# corresponds to the value of y_[i]. For example, if there were 43 classes and y_[i]\n",
    "# had a value of 3, the resulting one-hot encoding would be [0, 0, 0, 1, 0, 0, ..., 0 ,0]\n",
    "# where the \"...\" held 0s.\n",
    "one_hot_y = tf.one_hot(y_, 43)\n",
    "\n",
    "# define the probability that a neruons output is kept.\n",
    "# this variable relates to the technique of Drop Out, which aims to prevent the network\n",
    "# from overfitting on the training dataset.\n",
    "# By setting this variable as a placeholder, I can control it when evaluating network\n",
    "# performance and when testing network performance.\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell defines the architecture of the Convolutional Network\n",
    "# It is comprised of two convolutional layers and two fully connected layers.\n",
    "# The fully connected layers implement Drop Out to reduce overfitting.\n",
    "\n",
    "# Layer 1\n",
    "# input: [32, 32, 3]\n",
    "# output: [16, 16, 32]\n",
    "\n",
    "# Define the weights of the first convolutional layer. Weights are pulled from a normal\n",
    "# distribution with 0 mean and 0.1 for standard deviation.\n",
    "# These weights are changed during the training phase of the model and cause the \n",
    "# network to \"learn\" how to recognize German Traffic Signs.\n",
    "W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 32], stddev=0.1))\n",
    "\n",
    "# Define the bias to be a vector of values = 0.1. Like the weights, the bias is also\n",
    "# updated to help the network learn how to classify German Traffic Signs.\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "\n",
    "# tf.nn.conv2d will convolve the weights with the input image. When complete, the bias is added\n",
    "# and this addition is sent to relu activation units.\n",
    "conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1,1,1,1], padding='SAME') + b_conv1)\n",
    "\n",
    "# the output of the Rectified Linear Activation units is then fed into a max pooling layer with\n",
    "# a size of 2x2. Meaning that within a 2x2 square, only the maximum value will be kept and fed\n",
    "# onward into the network.\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# Layer 2\n",
    "# input: [16, 16, 32]\n",
    "# output: [8, 8, 64]\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.conv2d(conv1, W_conv2, strides=[1,1,1,1], padding='SAME'))\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# Layer 3 (fully connected)\n",
    "# input: [8, 8, 64]\n",
    "# output: 1024\n",
    "\n",
    "# The convolutional layer conv2 is a 4D tensor, with Width, Height, and Depth. This tensor\n",
    "# must be flattened prior to entering the fully connected layer. \n",
    "\n",
    "# reshape conv2\n",
    "fc0 = tf.reshape(conv2, [-1, 8 * 8 * 64])\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "fc1 = tf.nn.relu(tf.matmul(fc0, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout is implemented here. keep_probability defines the probability of a neurons layer being\n",
    "# kept. The choosing of which neuron to drop is random. Since dropout of a neuron is random,\n",
    "# this fully connected layer will literally take a unique shape for every iteration. Different\n",
    "# shapes allow for better generationalizion and less likely to overfit the training images.\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "# Layer 4 (output layer; softmax layer)\n",
    "# the output must correspond to the number of classes in this dataset.\n",
    "W_fc2 = tf.Variable(tf.truncated_normal(shape=[1024, 43], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[43]))\n",
    "y_conv = tf.matmul(fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax will assign probabilities to the input image belonging to one of the 43 classes.\n",
    "# Softmax will exponentialize its input and then normalize them, which forms a valid probability \n",
    "# distribution. \n",
    "\n",
    "# Cross-Entropy is used to measurement how badly the model is performing. This is called loss -- how far\n",
    "# off the model is from the desired outcome. Cross-Entropy is a very common loss function. \n",
    "\n",
    "# Tensorflow has a very efficient function that will obtain the softmax of the output and produce the \n",
    "# cross_entropy. tf.reduce_mean computes the average over all the examples in the batch.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=y_conv))\n",
    "\n",
    "# Here, I ask tensorflow to minimize cross_entropy loss through ADAM optimizer. This process\n",
    "# will use back-propagation to move the weights in the direction of greatest change. \n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# Here is were the model figures out if it predicted the correct label.\n",
    "# argmax will return the index of the largest value in the tensors one_hot_y and y_conv.\n",
    "# recall that in one_hot_y, all but 1 entries were 0. So the index with the value 1 will be\n",
    "# return (which corresponds to the label). y_conv is a tensor with shape 43. the index\n",
    "# of the element within y_conv with the greate value will be returned. the index corresponds\n",
    "# to the network's predicted label.\n",
    "# tf.equal will compare these indices and return True if they match, or False otherwise\n",
    "# As such, correct_prediction is a vector of [True, True, False, False, True, False, ...]\n",
    "correct_prediction = tf.equal(tf.argmax(one_hot_y, 1), tf.argmax(y_conv, 1))\n",
    "\n",
    "# To obtain accuracy, the model converts these booleans into their corresponding float values;\n",
    "# either 0 or 1, through tf.cast. Then, tf.reduce_mean returns the average of the vector which\n",
    "# corresponds to the accuracy of the network.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate is called only on the validation and testing datasets. Keep probability is set to 1, meaning \n",
    "# all neruons are present -- the fully connected layer will not change between successive batches.\n",
    "\n",
    "def evaluate(x_data, y_data):\n",
    "    \n",
    "    num_examples = len(x_data)\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    \n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        \n",
    "        # obtains a small batch from the larger dataset that is x_data and y_data\n",
    "        batch_x, batch_y = x_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        \n",
    "        # tell tensorflow session to evaluate accuracy. feed_dict will populate tensorflow\n",
    "        # placeholders and variables with the required data to run.\n",
    "        # notice how keep_prob is set to 1, indicating a 100% chance that any neruon's output will\n",
    "        # be kept.\n",
    "        local_accuracy = sess.run(accuracy, feed_dict={x_image: batch_x, y_: batch_y, keep_prob: 1.0}) \n",
    "        \n",
    "        total_accuracy += (local_accuracy * len(batch_x))\n",
    "    \n",
    "    return total_accuracy / num_examples \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # intialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        \n",
    "        # shuffle data for every epoch, so the model does not see the same\n",
    "        # series of images each time.\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "        \n",
    "        # BATCH_SIZE number of images will be fed into the network at a time.\n",
    "        # a large batch size results in faster training, but is more computationally\n",
    "        # intensive.\n",
    "        for offset in range(0, NUM_EXAMPLES, BATCH_SIZE):\n",
    "            \n",
    "            # set the offset\n",
    "            end = offset + BATCH_SIZE\n",
    "            \n",
    "            # obtain our current batches for this iteration\n",
    "            batch_x, batch_y = x_train[offset:end], y_train[offset:end]\n",
    "            \n",
    "            # train, using the ADAM optimizer to update network weights. \n",
    "            # 20% of neruons will be dropped.\n",
    "            train_step.run(feed_dict={x_image: batch_x, y_: batch_y, keep_prob: 0.8})\n",
    "        \n",
    "        # when finished training on the training data set, evalute network performance\n",
    "        # on the validation set. This occurs for every epoch.\n",
    "        validation_accuracy = evaluate(x_valid, y_valid)\n",
    "        print('epoch %d, training accuracy %g' % (i+1, validation_accuracy))\n",
    "    \n",
    "    # when training is complete, evaluate network performance on the test data set. The network only sees\n",
    "    # this dataset at the end of training and is an indicator to how well the network generalizes.\n",
    "    # if the network shows very high accuracy for validation and a low accuracy for testing,\n",
    "    # there is a good chance the network overfitted, may want to decrease keep_probability.\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x_image: x_test, y_: y_test, keep_prob: 1.0}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
